{
 "metadata": {
  "hex_info": {
   "author": "Diana Shaw",
   "exported_date": "Thu Feb 29 2024 20:20:21 GMT+0000 (Coordinated Universal Time)",
   "project_id": "b4861126-fb2c-4f2e-a1db-f725fca1baba",
   "version": "draft"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "9be5a4ba-56fd-4b09-911c-615917b7c48c",
   "metadata": {
    "language": "python",
    "name": "EndToEnd",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Import necessary functions\nimport streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Define image in a stage and read the file\nimage=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.ASSETS/End-to-end_demo.png\" , decompress=False).read() \n\n# Display the image\nst.image(image, width=1000)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "HOL_Background",
    "collapsed": false
   },
   "source": [
    "## Background Information\n",
    "\n",
    "Tasty Bytes is one of the largest food truck networks in the world with localized menu options spread across 30 major cities in 15 countries. **Tasty Bytes is aiming to achieve 25% YoY sales growth over 5 years.**\n",
    "\n",
    "As Tasty Bytes Data Scientists, we have been asked to support this goal by helping our food truck drivers more intelligently pick where to park for shifts. \n",
    "\n",
    "**We want to direct our trucks to locations that are expected to have the highest sales on a given shift.\n",
    "This will maximize our daily revenue across our fleet of trucks.**\n",
    "\n",
    "To provide this insight, we will use historical shift sales at each location to build a model. This data has been made available to us in Snowflake.Our model will provide the predicted sales at each location for the upcoming shift.\n",
    "\n"
   ],
   "id": "3fa8db12-24c0-4fd5-8387-9241b55371c7"
  },
  {
   "cell_type": "code",
   "id": "a3513b76-4e4d-4d61-afa4-a6e7cda02632",
   "metadata": {
    "language": "python",
    "name": "ProblemOverview",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "image=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.ASSETS/problem_overview.png\" , decompress=False).read()\nst.image(image, width=1000)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Libraries",
    "collapsed": false
   },
   "source": "## Import Packages\n\nJust like the Python packages we are importing, we will import the Snowpark modules that we need.\n\n**Value**: Snowflake modules provide efficient ways to work with data and functions in Snowflake.\n\n",
   "id": "63a66d83-6552-4390-8f4a-abecaffcd076"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Libraries_Code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Snowpark Imports\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.version import VERSION\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.exceptions import SnowparkSessionException\nfrom snowflake.snowpark.functions import sproc\nfrom snowflake.snowpark import types as T\nfrom snowflake.snowpark.functions import col\nfrom snowflake.snowpark.window import Window\n\n# Snowpark ML\nfrom snowflake.ml.modeling.impute import SimpleImputer\nfrom snowflake.ml.modeling.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.xgboost import XGBRegressor\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.modeling.metrics import mean_absolute_percentage_error\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\n\n# Snowflake Task API\nfrom snowflake.core import Root\nfrom snowflake.core.database import Database\nfrom snowflake.core.schema import Schema\nfrom snowflake.core.warehouse import Warehouse\nfrom snowflake.core import Root\nfrom snowflake.core.task import StoredProcedureCall\nfrom snowflake.core.task.dagv1 import DAG, DAGTask, DAGOperation\nfrom snowflake.core._common import CreateMode\n\n# Streamlit\nimport streamlit as st\n\n# Other Imports\nfrom datetime import timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sys\nimport json\n\n#\nimport warnings\nwarnings.filterwarnings(\"ignore\")",
   "id": "78669da6-cf98-4905-8ce7-4d107f566da2"
  },
  {
   "cell_type": "code",
   "id": "00b5e6c8-132b-47d3-a876-957d2fc7b926",
   "metadata": {
    "language": "python",
    "name": "HOL_Environment",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Create Snowflake Session object\nsession = get_active_session()\nsession.sql_simplifier_enabled = True\n\nsnowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\nsnowpark_version = VERSION\n\n# Current Environment Details\nprint('\\nConnection Established with the following parameters:')\nprint('User                        : {}'.format(snowflake_environment[0][0]))\nprint('Role                        : {}'.format(session.get_current_role()))\nprint('Database                    : {}'.format(session.get_current_database()))\nprint('Schema                      : {}'.format(session.get_current_schema()))\nprint('Warehouse                   : {}'.format(session.get_current_warehouse()))\nprint('Snowflake version           : {}'.format(snowflake_environment[0][1]))\nprint('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Part1_PrepareData",
    "collapsed": false
   },
   "source": "# Part 1 - Use Snowpark to access and prepare data for modeling",
   "id": "1d0c0e73-1049-4096-b72e-800a41185179"
  },
  {
   "cell_type": "code",
   "id": "b5d0268d-41dd-4e76-a4b2-8aa027ace3f5",
   "metadata": {
    "language": "python",
    "name": "Part1",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "image=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.ASSETS/Part1.png\" , decompress=False).read() \nst.image(image, width=1000)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1fd3b3f4-358e-40bd-8c44-2598cb992bdc",
   "metadata": {
    "name": "SnowflakeMLOverview",
    "collapsed": false
   },
   "source": "**[Snowflake ML](https://docs.snowflake.com/en/developer-guide/snowpark-ml/overview)** is an integrated set of capabilities for end-to-end machine learning in a single platform on top of your governed data. Snowflake ML can be used for both fully custom and out-of-the-box workflows.\n\nFor custom ML, data scientists and ML engineers can easily and securely develop and productionize scalable features and models without any data movement, silos, or governance tradeoffs. These custom ML capabilities can be accessed through Python APIs from the Snowpark ML library.\n\nCapabilities for custom ML include:\n\n- **[Snowflake Notebooks](https://docs.snowflake.com/en/user-guide/ui-snowsight/notebooks)** for a familiar, easy-to-use notebook interface that blends Python, SQL, and Markdown\n- **Container Runtimes** for distributed CPU and GPU processing out of the box from Snowflake Notebooks\n- **[Snowpark ML Modeling](https://docs.snowflake.com/en/developer-guide/snowpark-ml/modeling)** for feature engineering and model training with familiar Python frameworks\n- **[Snowflake Feature Store](https://docs.snowflake.com/en/developer-guide/snowpark-ml/feature-store/overview)** for continuous, automated refreshes on batch or streaming data\n- **[Snowflake Model Registry](https://docs.snowflake.com/en/developer-guide/snowpark-ml/model-registry/overview)** to manage models and their metadata\n- **ML Lineage** to trace end-to-end feature and model lineage (currently in private preview)"
  },
  {
   "cell_type": "code",
   "id": "4ab96dcc-ca3d-44bd-8e0b-8ab436365ddf",
   "metadata": {
    "language": "python",
    "name": "SnowflakeML_Overview",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "image=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.ASSETS/snowpark_ML.png\" , decompress=False).read() \nst.image(image, width=1000)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "SnowparkDataFrame",
    "collapsed": false
   },
   "source": "## Snowpark DataFrame\n\nLet's create a Snowpark DataFrame containing our shift sales data from the **shift_sales_v** view in our Snowflake account using the Snowpark session.table function. A DataFrame is a data structure that contains rows and columns, similar to a SQL table.\n\n**Value:** Familiar representation of data for Python users.\n\n",
   "id": "2c0f76db-e5c7-4d4a-92dc-f389f3e41c22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SnowparkDataFrame_Code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "snowpark_df = session.table(\"SHIFT_SALES\")",
   "id": "449b12ef-e1a4-4d2e-8925-b62628fbcf4e"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "SnowparkDataFrame_View",
    "collapsed": false
   },
   "source": "## Preview the Data\n\nWith our Snowpark DataFrame defined, let’s use the .show() function to take a look at the first 10 rows.\n\n**Value:** Instant access to data.\n\n",
   "id": "37430d96-c797-4afb-b6e9-1280d222415f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SnowparkDataFrame_View_Code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Preview the data using the .show() function to look at the first 10 rows.\nsnowpark_df.show()",
   "id": "56bb0880-3518-484e-a77d-cfbf1744da17"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "SnowparkDataFrame_Transformations",
    "collapsed": false
   },
   "source": [
    "## Select, Filter, Sort\n",
    "\n",
    "Notice the Null values for \"shift_sales\". Let's look at a single location.To do this, we will make another Snowpark DataFrame, location_df, from the above DataFrame and we will:\n",
    "\n",
    "1. Select columns\n",
    "2. Filter to a single location ID\n",
    "3. Sort by date\n",
    "\n",
    "**Value**: Efficient transformation pipelines using Python syntax and chained logic.\n",
    "\n"
   ],
   "id": "abdd91e2-64ff-4ba0-bbca-18c236b0dbfc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SnowparkDataFrame_Transformations_Code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "# Select\n",
    "location_df = snowpark_df.select(\"date\", \"shift\", \"shift_sales\", \"location_id\", \"city\")\n",
    "\n",
    "# Filter\n",
    "location_df = location_df.filter(F.col(\"location_id\") == 1135)\n",
    "\n",
    "# Sort\n",
    "location_df = location_df.order_by([\"date\", \"shift\"], ascending=[0, 0])\n",
    "\n",
    "# Display\n",
    "location_df.show(n=20)"
   ],
   "id": "2ee49ad3-0979-4363-8a43-0dc33e4c234c"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Snowpark_How",
    "collapsed": false
   },
   "source": "We can see that shift sales are populated 8 days prior to the latest date in the data. The **missing values** represent future dates that do not have shift sales yet.",
   "id": "cb6d2ad4-4c91-43a5-83ac-05c2800ffdce"
  },
  {
   "cell_type": "markdown",
   "id": "60423d08-f3c4-405d-9296-6d5229e7d44a",
   "metadata": {
    "name": "SnowparkMLModeling",
    "collapsed": false
   },
   "source": "## Snowpark ML Modeling \nSupports data preprocessing, feature engineering, and model training in Snowflake using popular machine learning frameworks, such as scikit-learn, xgboost, and lightgbm. This API also includes a preprocessing module that can use compute resources provided by a Snowpark-optimized warehouse to provide scalable data transformations."
  },
  {
   "cell_type": "code",
   "id": "f1472b6e-2120-45c2-8216-3322876c0f28",
   "metadata": {
    "language": "python",
    "name": "Snowpark",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "image1=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.ASSETS/Snowpark_ML_API.png\" , decompress=False).read() \nimage2=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.ASSETS/SnowparkValue.png\" , decompress=False).read() \n\n# Display the image\nst.image(image1, width=1000)\nst.subheader(\"Here's the value of using Snowpark:\")\nst.image(image2, width=1000)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Snowpark_ExplainQuery",
    "collapsed": false
   },
   "source": "## Explain the Query\n\nLet's look at what was executed in Snowflake to create our location_df DataFrame.\n\nThe translated SQL query can be seen in the Snowsight interface under _Activity_ in the _Query History_ or directly in our notebook by using the explain() function. \n\n**Value:** Transparent execution and compute usage.",
   "id": "08f1b00b-3560-49a4-a7ea-523fd16687c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Snowpark_Explain",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "location_df.explain()"
   ],
   "id": "8db7ed9c-b597-4737-8b09-43540b9d17b4"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "CompareSFDataFramevsPandasDataFrame",
    "collapsed": false
   },
   "source": [
    "## Compare DataFrame Size\n",
    "\n",
    "Let's bring a sample of our Snowflake dataset to our Python environment in a pandas DataFrame using the to_pandas() function. We will compare how much memory is used for the pandas DataFrame compared to the Snowpark DataFrame. As we will see, no memory is used for the Snowpark DataFrame in our Python environment. All data in the Snowpark DataFrame remains on Snowflake.\n",
    "**Value:** No copies or movement of data when working with Snowpark DataFrames.\n",
    "\n"
   ],
   "id": "0ce2b480-b880-4fdd-aa3c-3c94d7d0c0cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "CompareSFDataFramevsPandasDataFrame_Code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "# Bring 10,000 rows from Snowflake to pandas\n",
    "pandas_df = snowpark_df.limit(10000).to_pandas()\n",
    "\n",
    "# Get Snowpark DataFrame size\n",
    "snowpark_size = sys.getsizeof(snowpark_df) / (1024*1024)\n",
    "print(f\"Snowpark DataFrame Size (snowpark_df): {snowpark_size:.2f} MB\")\n",
    "\n",
    "# Get pandas DataFrame size\n",
    "pandas_size = sys.getsizeof(pandas_df) / (1024*1024)\n",
    "print(f\"Pandas DataFrame Size (pandas_df): {pandas_size:.2f} MB\")"
   ],
   "id": "3b78bc1f-4154-4288-b2b2-85c496a7829d"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Snowpark_DataExploration",
    "collapsed": false
   },
   "source": "## Data Exploration\n\nHere, we will use Snowpark to explore our data. A common pattern for exploration is to use Snowpark to manipulate our data and then bring an aggregate table to our Python environment for visualization.\n\n**Value:** - Native Snowflake performance and scale for aggregating large datasets. - Easy transfer of aggregate data to the client-side environment for visualization.\nAs we explore our data, we will highlight what is being done in Snowflake and what we are transferring to our client-side environment (Python notebook environment) for visualization.\n\n",
   "id": "eb800752-b83e-4340-9e1d-d3da732c7fb9"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "SnowparkDF_Count",
    "collapsed": false
   },
   "source": "## How many rows are in our data?\n\nThis will give us an idea of how we might need to approach working with this data. Do we have enough data to build a meaningful model? What compute might be required? Will we need to sample the data?\n\n**What's happening where?:** Rows counted in Snowflake. No data transfer.\n\n",
   "id": "276e6283-133c-4531-bf10-b1314521b1e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SnowparkDF_Count_Code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "#Use the .count() function\n",
    "snowpark_df.count()"
   ],
   "id": "d57909f6-d03b-4f12-9ca0-e831ed44107f"
  },
  {
   "cell_type": "code",
   "id": "09dfe67a-ce33-425c-a25e-b55893b02c8d",
   "metadata": {
    "language": "python",
    "name": "ExploreSnowparkDFwithSiS",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Add a select slider to choose the day of the week in the first column\ndays_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nselected_day = st.select_slider(\"Select Day of the Week\", options=list(range(7)), format_func=lambda x: days_of_week[x])\n\n# Filter the dataframe based on the selected day of the week\nfiltered_df = snowpark_df.filter(col(\"day_of_week\") == selected_day)\n\n# Group by city and calculate the average shift sales\ndf = filtered_df.group_by(\"city\").agg(F.mean(\"shift_sales\").alias(\"avg_shift_sales\"))\n\n# Plot the data using Streamlit's bar_chart\nst.bar_chart(data=df,x='CITY',y='AVG_SHIFT_SALES')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Snowpark_Describe",
    "collapsed": false
   },
   "source": "## Let's calculate some descriptive statistics.\n\nWe use the Snowpark describe() function to calculate summary statistics and then bring the aggregate results into a pandas DataFrame to visualize in a formatted table.\n\n**What's happening where?:** Summary statistics calculated in Snowflake. Transfer aggregate summary statistics for client-side visualization.\n\n",
   "id": "271fb4e5-0907-4d0d-bb63-85a79b702f44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Snowpark_Describe_Code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#Use the Snowpark DataFrame .describe function. You need to need to visualize from a pandas DataFrame\nsnowpark_df.describe().to_pandas()",
   "id": "8a73b888-fb61-46c2-8571-98b7e9932b43"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Snowpark_FeatureEngineering",
    "collapsed": false
   },
   "source": "## Feature Engineering\n\nNow let's keep revelant columns and transform columns to create features needed for our prediction model.To make some of our features more useful, we will normalize them using standard preprocessing techniques, such as One-Hot Encoding and MinMaxScaling. With SnowparkML, you can use a standard sklearn-style API to execute fully distributed feature engineering preprocessing tasks on Snowflake compute, with zero data movement. Let's fit a scaler and encoder to our data, then use it to transform the data, producing new feature columns.\n\n\n**Value:** The Snowpark syntax makes pipelines easy to implement and understand. The syntax also allows for easy migration of Spark pipelines to Snowflake.\n\n\n**All transformations for feature engineering in this notebook will be executed on Snowflake compute.**\n\nNotice what we haven't had to do? No tuning, maintenance, or operational overhead. We just need a role, warehouse, and access to the data.\n\n**Value**: Near-zero maintenance. Focus on the work that brings value.\n\n",
   "id": "d2bd04e5-acfa-4f3a-9fab-fa73c285685b"
  },
  {
   "cell_type": "code",
   "id": "23836722-ae78-4620-bd2d-d4d9f1fcd6e7",
   "metadata": {
    "language": "python",
    "name": "Part1B",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "image1=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.ASSETS/Part1b.png\" , decompress=False).read() \nst.image(image1, width=1000)\nimage2=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.ASSETS/SFFeatureStore.png\" , decompress=False).read() \nst.image(image2, width=1000)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f6c663f9-62d8-4113-aa74-b4e16295d810",
   "metadata": {
    "name": "SnowflakeFeatureStore",
    "collapsed": false
   },
   "source": "In the **[Snowflake Feature Store](https://docs.snowflake.com/en/developer-guide/snowpark-ml/feature-store/overview)**, as typical of other Feature Store solutions:\n\n- **Entities** - define the business-entity and the level that we want to gather data and develop ML models at. (e.g. store or/and product key etc).\n- **Features** are defined and grouped within **FeatureViews**. In Snowflake Feature Store features are columns, or column-expressions defined via the Snowpark for Python dataframe api, or via SQL directly.\n- **FeatureViews** are associated (defined) for one or more **Entities**. A **FeatureView** can be defined with 1:n Entities, but typically only one. Several (many) **FeatureViews** may contain Features for the same Entity. FeatureViews tend to get defined based on the data-source they are derived from, the data's refresh or calculation frequency. A **FeatureView** is defined via a Snowpark Dataframe (or SQL expression) enabling a complex pipeline to be used.\n- The **Entity** (key columns) are used to join **FeatureViews** together when needed to gather features from multiple **FeatureViews** within a single training or inference dataset, or derive new **FeatureViews**.\n- A **FeatureSlice** provides a way of creating a subset of the Features from a single **FeatureViews** when needed. It can be used within the API, pretty much anywhere the **FeatureViews** can be used.\n- **FeatureViews** and **FeatureSlices** can be merged (via merge_features) to gather features together and create a new **FeatureView**. For example, all the features for a given **Entity** could be gathered via the merge into a single."
  },
  {
   "cell_type": "markdown",
   "id": "087f9578-9823-455b-9e58-b971ff488a16",
   "metadata": {
    "name": "FeatureStore_Client",
    "collapsed": false
   },
   "source": "### Creating and Registering the Feature Store and Entity\nBefore we can calculate and register our features, we need to create a feature store and define an entity that encapsulates the keys used to join features.\n- **Feature Store Creation:** The feature store is created within a specified database and schema, and it is configured to create if it does not already exist.\n- **Entity Definition:** An entity named AGGREGATE_WINDOW is defined, which includes the join keys location_id and shift that will be used for feature lookups.\n- **Entity Registration:** The entity is registered in the feature store if it doesn't already exist. If it does exist, the existing entity is retrieved."
  },
  {
   "cell_type": "code",
   "id": "7e82f4f0-1e5b-40b1-9e02-4f930c783c27",
   "metadata": {
    "language": "python",
    "name": "CreateFeatureStore",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Import necessary libraries and create session\nfrom snowflake.ml.feature_store import FeatureStore, CreationMode\n\n# Create a Feature Store\nfs = FeatureStore(\n    session=session,\n    database=session.get_current_database(),\n    name=session.get_current_schema(),\n    default_warehouse=session.get_current_warehouse(),\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n)\n\nprint(\"FeatureStore created or referenced successfully.\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9374a9e0-bdf0-4389-9af3-63a471200c68",
   "metadata": {
    "language": "python",
    "name": "DefineRegisterEntity",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Define the name and keys for the AGGREGATE_WINDOW entity\naggregate_window_entity_name = \"AGGREGATE_WINDOW\"\naggregate_window_entity_join_keys = [\"location_id\", \"shift\"]\n\n# List existing entities in the feature store\nexisting_entities = json.loads(fs.list_entities().select(F.to_json(F.array_agg(\"NAME\", True))).collect()[0][0])\n\n# Check if the AGGREGATE_WINDOW entity already exists\nif aggregate_window_entity_name not in existing_entities:\n    # Create the AGGREGATE_WINDOW entity\n    aggregate_window_entity = Entity(\n        name=aggregate_window_entity_name, \n        join_keys=aggregate_window_entity_join_keys, \n        desc=\"Aggregate window for rolling shift average by location\"\n    )\n    fs.register_entity(aggregate_window_entity)\nelse:\n    # Get the existing AGGREGATE_WINDOW entity\n    aggregate_window_entity = fs.get_entity(aggregate_window_entity_name)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "886c48b5-49f2-4c52-af57-3f1f6ad9ed53",
   "metadata": {
    "name": "FeatureView",
    "collapsed": false
   },
   "source": "### Create a new FeatureView and materialize the feature pipeline\nNow we construct a Feature View with above snowpark_df DataFrame. First, we create a draft feature view (fv). We set the refresh_freq to 30 days which will create a Snowflake dynamic table . At this point, the draft feature view will not take effect because it is not registered yet. Then we register the feature view by via register_feature_view. It will materialize to Snowflake backend. Incremental maintenance will start if the query is supported."
  },
  {
   "cell_type": "markdown",
   "id": "ca0011f3-f2aa-4be7-9290-f2d2f174582f",
   "metadata": {
    "name": "Entity",
    "collapsed": false
   },
   "source": "### Aggregating Across the Window\nNext, we'll define a rolling average feature that calculates the average shift sales across all previous days for each location and shift. This feature will be used in our ML model development.\n- Window Definition: A window function is defined to calculate the rolling average of shift sales for each location and shift.\n- Feature Creation: The rolling average is calculated using the Snowpark DataFrame API and added as a new column in the DataFrame.\n- Feature View Definition: A feature view is created to encapsulate this feature, and it is registered in the feature store. The timestamp_col ensures that the feature is treated as a time-series feature, and the refresh_freq determines how often it gets updated."
  },
  {
   "cell_type": "code",
   "id": "1b6ca6d7-8e9f-44e1-bc71-8d75f07e86a9",
   "metadata": {
    "language": "python",
    "name": "CreateRegisterFeatureView",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.feature_store import FeatureView\nfrom snowflake.snowpark import Window\nimport snowflake.snowpark.functions as F\n\n# Define the window for calculating the rolling shift average by location\nwindow_by_location_all_days = (\n    Window.partition_by('\"LOCATION_ID\"', '\"SHIFT\"')\n    .order_by('\"DATE\"')\n    .rows_between(Window.UNBOUNDED_PRECEDING, Window.CURRENT_ROW - 1)\n)\n\n# Use the window to create an aggregate across the window\nsnowpark_df = snowpark_df.with_column(\n    \"avg_location_shift_sales\", \n    F.avg(\"shift_sales\").over(window_by_location_all_days)\n)\n\n# Define the feature view\nfeature_view = FeatureView(\n    name=\"AGGREGATE_WINDOW_FV\",\n    entities=[aggregate_window_entity],   # Use the entity created above\n    feature_df=snowpark_df,               # This is your Snowpark DataFrame with features\n    timestamp_col=\"DATE\",                 # Use your timestamp column here\n    refresh_freq=\"30 days\",               # Set the refresh frequency according to your needs\n    desc=\"TastyBytes rolling shift average by location_id refreshed on a schedule\"\n)\n\n# Register the feature view\nregistered_fv = fs.register_feature_view(\n    feature_view=feature_view,\n    version=\"1\",\n    block=True\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fcfb7d97-8bfd-4ac1-b4ad-11df725c8057",
   "metadata": {
    "language": "python",
    "name": "SnowsightUIUpdated",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "image1=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.ASSETS/feature_store.png\" , decompress=False).read() \nst.image(image1, width=1000)\n\n# Display what HOL should viusalize\nst.subheader(\"Here is what your Snowpark Feature should look like:\")\nimage2=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.ASSETS/HOL_Example.png\" , decompress=False).read() \nst.image(image2, width=1000)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Snowpark_ImputeMIssingValues",
    "collapsed": false
   },
   "source": [
    "## Impute Missing Values\n",
    "\n",
    "The rolling average feature we just created is missing if there are no prior shift sales at that location. We will replace those missing values with 0.\n",
    "\n"
   ],
   "id": "dc1cefbe-a91b-45d8-a22a-aa52b6ebe838"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Snowpark_ImputeMIssingValues_Code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Fill NaN values in the avg_location_shift_sales column with 0\nsnowpark_df = snowpark_df.fillna(value=0, subset=[\"avg_location_shift_sales\"])\n\n# Show the DataFrame to see the filled NaN values\nsnowpark_df.show()",
   "id": "523adbc7-205c-463f-acd2-d73c8cb1889c"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "SnowparkMLModelingAPI",
    "collapsed": false
   },
   "source": "## Leverage Snowpark ML Modeling API to create features\n\nSnowpark ML provides APIs to support each stage of an end-to-end machine learning development and deployment process and includes two key components: [Snowpark ML Modeling](https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-modeling) and [Snowpark ML Ops](https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-mlops-model-registry).\n\n[Snowpark ML Modeling](https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-modeling) supports data preprocessing, feature engineering, and model training in Snowflake using popular machine learning frameworks, such as scikit-learn, xgboost, and lightgbm. This API also includes a preprocessing module that can use compute resources provided by a Snowpark-optimized warehouse to provide scalable data transformations.\n\nSnowpark ML Operations (MLOps), featuring the [Snowpark ML Model Registry](https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-mlops-model-registry), complements the Snowpark ML Development API. The model registry allows secure deployment and management of models in Snowflake, and supports models trained both inside and outside of Snowflake.",
   "id": "5b8d2565-0726-4f1e-a842-edff14425bac"
  },
  {
   "cell_type": "code",
   "id": "d890e654-de61-43ff-bce6-c8dce6f3b6ad",
   "metadata": {
    "language": "python",
    "name": "Snowpark_4DE_DS",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "image=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.ASSETS/Snowpark4DE.png\" , decompress=False).read()\n\n# Display the image\nst.subheader(\"Here are a Snowpark Features commonly used for Data Engineering tasks:\")\nst.image(image, width=1000)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "AlterWarehouseSizeUp",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.sql(\"ALTER WAREHOUSE \" + session.get_current_warehouse() + \" SET WAREHOUSE_SIZE = 'LARGE'\").collect()",
   "id": "a0ade1af-883d-4843-bf9d-6fa61a449fa0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SnowparkMLModelingAPI_Code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Snowpark ML: Machine Learning Toolkit for Snowflake\n",
    "import snowflake.ml.modeling.preprocessing as snowmlpp\n",
    "\n",
    "# Define our scaler and ordinal encoding functions\n",
    "\n",
    "# Snowpark ML scaler (MinMaxScaler) is used to shrink data within the given range, usually of 0 to 1. \n",
    "# It transforms data by scaling features to a given range. It scales the values to a specific value range without changing the shape of the original distribution.\n",
    "# For the Tasty_Bytes data, use MinMaxScaler to normalize \"CITY_POPULATION\" into \"CITY_POPULATION_NORM\" with values between 0 and 1.\n",
    "\n",
    "def fit_scaler(session, df):\n",
    "    mm_target_columns = [\"CITY_POPULATION\"]\n",
    "    mm_target_cols_out = [\"CITY_POPULATION_NORM\"]\n",
    "    snowml_mms = snowmlpp.MinMaxScaler(input_cols=mm_target_columns, output_cols=mm_target_cols_out)\n",
    "    snowml_mms.fit(df)\n",
    "    return snowml_mms\n",
    "\n",
    "# Snowpark ML ordinal encoding (OE) is used to improve model performance by providing more information to the model about categorical variables. \n",
    "# It can help to avoid the problem of ordinality, which can occur when a categorical variable has a natural ordering (e.g. “small”, “medium”, “large”).\n",
    "# For the Tasty_Bytes data, use OE to change \"SHIFT\" which is currently AM or PM into and integer representation of \"SHIFT_OHE\" is 1.0 or 0.0. \n",
    "\n",
    "def fit_oe(session, df):\n",
    "    oe_target_cols = [\"SHIFT\"]\n",
    "    oe_output_cols = [\"SHIFT_OE\"]\n",
    "    snowml_oe = snowmlpp.OrdinalEncoder(input_cols=oe_target_cols, output_cols=oe_output_cols)\n",
    "    snowml_oe.fit(df)\n",
    "    return snowml_oe"
   ],
   "id": "f2e664e2-e37a-4599-bef3-bd47d870732a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SnowparkMLModelingAPI_PreprocessingFunctionsCode",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "# Run Snowpark ML preprocessing functions against our feature data\n",
    "\n",
    "# For the Tasty_Bytes data, use MinMaxScaler to normalize \"CITY_POPULATION\" into \"CITY_POPULATION_NORM\" with values between 0 and 1.\n",
    "snowml_mms = fit_scaler(session, snowpark_df)\n",
    "normed_df = snowml_mms.transform(snowpark_df)\n",
    "\n",
    "# For the Tasty_Bytes data, use OneHotEncoder to change \"SHIFT\" which is currently AM or PM into and integer representation of \"SHIFT_OHE_AM\" is 1 or 0 and \"SHIFT_OHE_PM\" is 1 or 0. \n",
    "snowml_oe = fit_oe(session, normed_df)\n",
    "oe_df = snowml_oe.transform(normed_df)\n",
    "oe_df.show()"
   ],
   "id": "7bef3b90-ee88-4202-a0c5-07d04d9fe691"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Snowpark_CreateHoldoutData",
    "collapsed": false
   },
   "source": [
    "## Filter to Historical Data\n",
    "\n",
    "Our data includes placeholders for future data with missing shift sales. The future data represents the next 7 days of shifts for all locations. The historical data has shift sales for all locations where a food truck parked during a shift. We will only use historical data when training our model and will filter out the dates where the shift_sales column is missing.\n",
    "\n"
   ],
   "id": "2ae7ff76-59bc-4aea-adad-90a8acbaf60e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Snowpark_CreateHoldoutData_Code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Science best practice: Always perform data quality on your training set e.g. remove nulls or invalid cells as they are the biggest problem in a training set as they output high false positives\n",
    "\n",
    "# Specifically for Tasty_Bytes data, dates where \"shift_sales\" are null values reflect future dates where sales need to be predicted.\n",
    "# Filter out these future dates so these records will not be used in model training. \n",
    "historical_df = oe_df.filter(F.col(\"shift_sales\").is_not_null())"
   ],
   "id": "411ec9ff-3751-4b71-b5ac-4be29cef27d3"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Snowpark_PersistTransformations",
    "collapsed": false
   },
   "source": [
    "## Persist Transformations\n",
    "\n",
    "If we want to save the changes we can either save it as a table, meaning the SQL generated by the DataFrame is executed and the result is stored in a table or as a view where the DataFrame SQL will be the definition of the view.\n",
    "**save_as_table** saves the result in a table, if **mode='overwrite'** then it will also replace the data that is in it.\n",
    "\n"
   ],
   "id": "b77f1264-42f7-4f7f-993a-07d143e15e72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Snowpark_PersistTransformations_Code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Let's select \nhistorical_df.write.save_as_table(table_name='DATASCIENCECOLLEGE.PUBLIC.INPUT_DATA', mode='overwrite')\nsession.table('DATASCIENCECOLLEGE.PUBLIC.INPUT_DATA').show()",
   "id": "e19f47fa-281c-49c0-8f45-d9630662dafb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Part2_TrainMLModel",
    "collapsed": false
   },
   "source": "# Part 2 - Use Snowpark to train a model",
   "id": "cd41e298-f6d3-4b8c-9206-ebb214565053"
  },
  {
   "cell_type": "code",
   "id": "b208ffd0-7755-4d04-8e7c-d2ab48e60fe6",
   "metadata": {
    "language": "python",
    "name": "Part2",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "image=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.ASSETS/Part2.png\" , decompress=False).read() \n\n# Display the image\nst.image(image, width=600)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Snowpark_ModelPrep",
    "collapsed": false
   },
   "source": [
    "## Drop Columns\n",
    "\n",
    "Let's return to the original prepared table, with all cities listed, and drop columns that will not be used in the model.\n",
    "\n"
   ],
   "id": "80a40aff-0039-4bda-ab7b-ff98ff42ca1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Snowpark_ModelPrep_Code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "prepared_df = historical_df.drop(\"location_id\", \"city_population\", \"shift\", \"city\", \"date\")\n",
    "prepared_df.show()"
   ],
   "id": "2a93dfbd-ad07-46c8-b917-defd5bdf61fa"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Snowpark_XGBoostRegressorModel",
    "collapsed": false
   },
   "source": [
    "## Build a simple XGBoost Regression Model on Snowflake\n",
    "\n",
    "We will now use our training data to train a linear regression model on Snowflake.Recall from above, the two main ways that Snowpark works:\n",
    "\n",
    "1. Snowpark code translated and executed as SQL on Snowflake\n",
    "2. Python functions deployed in a secure sandbox in Snowflake\n",
    "\n",
    "We will be leveraging the deployment of Python functions into Snowflake for training and model deployment.\n",
    "\n"
   ],
   "id": "db7f8fa5-bce9-44aa-b25f-845865d59f79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Snowpark_XGBoostRegressorModelSetup_Code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Let's define relevant features needed for the prediction model.\nLABEL_COLUMNS = [\"SHIFT_SALES\"]\nOUTPUT_COLUMNS = [\"PRED_SHIFT_SALES\"]\nFEATURE_COLUMN_NAMES = [\"SHIFT_OE\", \"CITY_POPULATION_NORM\", \"MONTH\", \"DAY_OF_WEEK\",\"LATITUDE\",\"LONGITUDE\",\"AVG_LOCATION_SHIFT_SALES\"]\n\ninput_df = prepared_df.select(*LABEL_COLUMNS, *FEATURE_COLUMN_NAMES)\ninput_df.show()",
   "id": "caa7e956-b69f-4437-bb15-d95c99b89944"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "SnowparkML",
    "collapsed": false
   },
   "source": "[Snowpark ML Modeling](https://docs.snowflake.com/en/developer-guide/snowpark-ml/modeling) also includes metric calculations such as correlations, and more. We will use the [snowflake.ml.modeling.metrics](https://docs.snowflake.com/en/developer-guide/snowpark-ml/reference/latest/index) includes a correlation method on our input dataframe to identify any linearly correlated features to the output. We'll also use matplotlib to plot the resulting matrix. Notice that all of the correlation calculations are pushed down to Snowflake!\n\n",
   "id": "b18c401c-6e4c-4b45-a684-f11f8c0bb323"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SnowparkML_Correlation_Code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom snowflake.ml.modeling.metrics.correlation import correlation\ncorr_df = correlation(df=input_df)\n\nfig, ax = plt.subplots(10,8)\nsns.heatmap(corr_df.corr(), ax=ax, annot=True)",
   "id": "31a22f10-17ee-4129-a3b7-3b6f5fe8e8fe"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "SnowparkML_SplitData4Modeling",
    "collapsed": false
   },
   "source": "What's great about this, is that we are using a lot of Snowpark components under the hood- the dataframe API, SQL, Python stored procedures and more. But with the new [Snowpark ML API](https://docs.snowflake.com/en/developer-guide/snowpark-ml/reference/latest/index), data scientists can take advantage of all that Snowpark affords them, while using common, familiar APIs that match how they do their work today.\n\nNow that we have our feature data, let's actually fit an XGBoost model to our features to attempt to predict future sales. We'll fit several different models with different hyperparameters, and then show how we can use the Snowpark Model Registry to select our best-fit model.\n\n",
   "id": "8139efe5-f965-46cb-8bd7-6dfa0798ae9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SnowparkML_SplitData4Modeling_Code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Split the data into train and test sets\ntrain_df, test_df = input_df.random_split(weights=[0.9, 0.1], seed=98)",
   "id": "c82c4134-289e-4482-b24e-bfc57ef5071c"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "SnowparkMLAPI_Modeling",
    "collapsed": false
   },
   "source": "## What's happening when you leverage Snowpark ML Modeling API?\n\nLet's run our training job using the SnowparkML Modeling API- this will push down our model training to run on Snowflake, and you'll notice that the type of the model object returend is a SnowparkML XGBClassifier- this has some benefits, but also is fully compatible with the standard sklearn/xgboost model objects.\n\n- The model.fit() function actually creates a temporary stored procedure in the background. This also means that the model training is a single-node operation. Be sure to use a Snowpark Optimized Warehouse if you need more memory. We are just using an XS Standard Virtual Warehouse here, which we created at the beginning of this quickstart.\n- The model.predict() function actually creates a temporary vectorized UDF in the background, which means the input DataFrame is batched as Pandas DataFrames and inference is parallelized across the batches of data. You can check the query history once you execute the following cell to check.\n\nThe Snowpark ML Modeling API enables the use of popular Python ML frameworks, such as scikit-learn and XGBoost, for feature engineering and model training without the need to move data out of Snowflake. \n\nBenefits of Snowpark ML Modeling: \n- **Feature engineering and preprocessing:** Improve performance and scalability with distributed execution for common scikit-learn preprocessing functions. \n- **Model training:** Accelerate model training for scikit-learn, XGBoost and LightGBM models without the need to manually create stored procedures or user-defined functions (UDFs), and leverage distributed hyperparameter optimization.\n\nBehind the scenes, Snowpark ML parallelizes data processing operations by taking advantage of Snowflake’s scalable computing platform.",
   "id": "edac01ae-6e20-41ff-8377-e9fed0014eb7"
  },
  {
   "cell_type": "code",
   "id": "5f3d7302-301c-47a4-b6bc-234cd359ff67",
   "metadata": {
    "language": "python",
    "name": "SnowparkMLAPI",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "image=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.ASSETS/DistributedDataProcessing.png\" , decompress=False).read() \n\n# Display the image\nst.image(image, width=1000)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SnowparkMLAPI_Modeling_Code",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.xgboost import XGBRegressor\n# Define the XGBRegressor\nregressor = XGBRegressor(\n    label_cols = LABEL_COLUMNS,\n    input_cols = FEATURE_COLUMN_NAMES,\n    output_cols = OUTPUT_COLUMNS\n)\n\n# Train\n_ = regressor.fit(train_df)\n\n# Predict\n# result = regressor.predict(test_df)",
   "id": "c3179b2b-be59-41e2-961c-449efb2def2a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Snowpark_MLAPI_ModelPredictOutput_Code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Just to illustrate, we can also pass in a Pandas DataFrame to Snowpark ML's model.predict()\nregressor.predict(test_df.to_pandas())",
   "id": "e809f760-f997-44d8-9c84-ee63a29c71d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SnowparkMLAPI_ModelAccuracy_Code",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Let's analyze the results using Snowpark ML's MAPE\n# Use Snowpark ML metrics to calculate\nfrom snowflake.ml.modeling.metrics import mean_absolute_percentage_error, mean_squared_error\n\n# Predict\nresults = regressor.predict(test_df)\n\n# Calculate MAPE\nmape = mean_absolute_percentage_error(df=results, y_true_col_names=LABEL_COLUMNS, y_pred_col_names=OUTPUT_COLUMNS)\n\n# Calculate MSE\nmse = mean_squared_error(df=results, y_true_col_names=LABEL_COLUMNS, y_pred_col_names=OUTPUT_COLUMNS)\n\nresults.select([*LABEL_COLUMNS, *OUTPUT_COLUMNS]).show()\nprint(f'''Mean absolute percentage error: {mape}''')\nprint(f'''Mean squared error: {mse}''')",
   "id": "85674393-aeed-406c-a802-db292b597454"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SnowparkMLAPI_ModelAccuracy_SiS",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Plot actual vs predicted \ng = sns.relplot(data=results[\"SHIFT_SALES\", \"PRED_SHIFT_SALES\"].to_pandas().astype(\"float64\"), x=\"SHIFT_SALES\", y=\"PRED_SHIFT_SALES\", kind=\"scatter\")\ng.ax.axline((0,0), slope=1, color=\"r\")",
   "id": "957b744c-12d9-4e60-9542-5a6fffd26379"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "SnowparkMLAPI_GridSearch"
   },
   "source": [
    "## Snowpark ML's GridSearchCV()\n",
    "\n",
    "Now, let's use Snowpark ML's GridSearchCV() function to find optimal model parameters.\n",
    "\n"
   ],
   "id": "bd32172b-7aab-48bc-9852-8b2ff136ee46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SnowparkMLAPI_GridSearch_Code",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.model_selection import GridSearchCV\n\ngrid_search = GridSearchCV(\nestimator=XGBRegressor(),\nparam_grid={\n\"n_estimators\":[25, 50],\n\"learning_rate\":[0.4, 0.5],\n},\nn_jobs = -1,\nscoring=\"neg_mean_absolute_percentage_error\",\ninput_cols=FEATURE_COLUMN_NAMES,\nlabel_cols=LABEL_COLUMNS,\noutput_cols=OUTPUT_COLUMNS\n)\n_ = grid_search.fit(train_df)",
   "id": "22005572-4302-43d8-982e-ef903a879024"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SnowparkMLAPI_GridSearch_ModelImprovement_Code",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "import warnings\n\n# Suppress specific FutureWarning\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Let's analyze the grid_search results\ngs_results = grid_search.to_sklearn().cv_results_\nn_estimators_val = []\nlearning_rate_val = []\nfor param_dict in gs_results[\"params\"]:\n    n_estimators_val.append(param_dict[\"n_estimators\"])\n    learning_rate_val.append(param_dict[\"learning_rate\"])\nmape_val = gs_results[\"mean_test_score\"] * -1\n\ngs_results_df = pd.DataFrame(data={\n    \"n_estimators\": n_estimators_val,\n    \"learning_rate\": learning_rate_val,\n    \"mape\": mape_val\n})\n\n# Convert inf values to NaN\ngs_results_df.replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n\n# Plot the results\ng2 = sns.relplot(data=gs_results_df, x=\"learning_rate\", y=\"mape\", hue=\"n_estimators\", kind=\"line\")",
   "id": "0ad4f16a-8baf-470e-b0fc-d7d13e128990"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SnowparkMLAPI_OptimalModel_Code",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Let's save our optimal model and its metadata:\noptimal_model = grid_search.to_sklearn().best_estimator_\noptimal_n_estimators = grid_search.to_sklearn().best_estimator_.n_estimators\noptimal_learning_rate = grid_search.to_sklearn().best_estimator_.learning_rate\n\n\noptimal_mape = gs_results_df.loc[(gs_results_df['n_estimators']==optimal_n_estimators) &\n                        (gs_results_df['learning_rate']==optimal_learning_rate),'mape'].values[0]\n\nprint(optimal_model)\nprint(optimal_mape)\nprint(optimal_n_estimators)\nprint(optimal_learning_rate)",
   "id": "bc418ad8-c865-401e-b5a3-60647ae00954"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "ScaleDownWarehouse"
   },
   "source": [
    "## **Scale down your assigned Snowflake compute warehouse.**\n",
    "\n"
   ],
   "id": "8ba3f0db-5bd5-4673-8f31-97887b0a11fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "ScaleDownWarehouse_Code",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Decrease size of Snowflake compute warehouse to XSMALL\nsession.sql(\"ALTER WAREHOUSE \" + session.get_current_warehouse() + \" SET WAREHOUSE_SIZE = 'XSMALL'\").collect()",
   "id": "7afd018d-eb9f-408d-8b34-c2721cf4bbcb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Part3_ModelRegistry",
    "collapsed": false
   },
   "source": "# Part 3 - Use Snowpark for MLOps",
   "id": "696b4aad-2ab7-4b60-ac9d-a4d3234a2895"
  },
  {
   "cell_type": "code",
   "id": "7cce1de4-5316-44a7-8993-1e9871a27a3f",
   "metadata": {
    "language": "python",
    "name": "Part3",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "image=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.ASSETS/Part3.png\" , decompress=False).read() \n\n# Display the image\nst.image(image, width=1000)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "SnowflakeModelRegistry",
    "collapsed": false
   },
   "source": "## Now let's use Snowflake Model Registry (Snowpark ML Ops)\n\nModel Registry was created to support model management operations including model registration, versioning, metadata and audit trails. Integrated deployment infrastructure for batch inference is a critical ease-of-use feature. Users can deploy ML models for batch inference from the registry directly into a Snowflake Warehouse as a vectorized UDF, or as a service to a customer-specified Compute Pool in Snowpark Container Services.\n\nSnowflake's Model Registry supports SciKitLearn, XGBoost, Pytorch, Tensorflow and MLFlow (via the pyfunc interface) models.\n\nModel Registry allows easy deployment of pre-trained open-source models from providers such as HuggingFace. See this [blog](https://www.snowflake.com/blog/accelerate-ml-workflow-python-snowpark-ml/) for more detailsor the [Snowflake Model Registry documentation](https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-mlops-model-registry)",
   "id": "5dbfe1d1-fbf4-429d-9a8f-0b9e2e5dccf3"
  },
  {
   "cell_type": "code",
   "id": "8270739e-eb7d-4a4f-9eb6-fcf1a22de4d1",
   "metadata": {
    "language": "python",
    "name": "ModelRegistry",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "image=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.ASSETS/ModelRegistry.png\" , decompress=False).read() \n\n# Display the image\nst.image(image, width=1000)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5cdc7c40-1171-47b9-a505-7cd781fc987c",
   "metadata": {
    "language": "python",
    "name": "CreateModelRegistry",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Create the Model Registry and register your initial model\nfrom snowflake.ml.registry import Registry\n\nregistry = Registry(session=session, database_name=\"DATASCIENCECOLLEGE\", schema_name=\"PUBLIC\")\nmodel_name = \"SHIFT_SALES_PREDICTION\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1bde2057-bc12-4554-90d0-4c3b70868f3a",
   "metadata": {
    "language": "python",
    "name": "SnowparkModelRegisty_GetModelToBeRegistered_Code",
    "collapsed": false
   },
   "outputs": [],
   "source": " # Here is the code to get the latest version of the model named \"SHIFT_SALES_PREDICTION\" within the Snowflake Model Registry\ndef get_next_model_version_to_be_registered(model_name:str):\n    def remove_char(lst, char):\n        import re\n        from functools import reduce\n        return reduce(lambda x, y: x + [int(re.sub(char, '', y))], lst, [])\n    model_version_to_be_registered = 0\n    try:\n        m = registry.get_model(model_name)\n        model_version_to_be_registered = max(remove_char(m.show_versions()['name'].tolist(),'V'))+1\n    except:\n        pass\n    return model_version_to_be_registered",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SnowflakeModelRegistry_CreateRegisterModel_OriginalModel_Code",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Log the model to the registry\nmodel_ver = registry.log_model(\n    model_name=model_name,\n    version_name=f'V{get_next_model_version_to_be_registered(model_name)}',\n    conda_dependencies=[\"xgboost\"],\n    model=regressor,\n    comment=\"This is the initial model of the Shift Sales Price Prediction model.\",\n    sample_input_data=train_df.drop(\"SHIFT_SALES\"),\n    options={'relax_version': False})",
   "id": "703db0dc-670a-4d44-8459-4fd0f77d3e53"
  },
  {
   "cell_type": "code",
   "id": "f2ced973-6ea0-4cbe-9d1b-98ceae148ce6",
   "metadata": {
    "language": "python",
    "name": "SnowflakeModelRegistry_RegisterOptimalModel_Code",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Log the optimal model to the registry\nmodel_ver2 = registry.log_model(\n    model_name=model_name,\n    version_name=f'V{get_next_model_version_to_be_registered(model_name)}',\n    model=optimal_model,\n    comment=\"This is the optimal model Shift Sales Price Prediction model developed using hyperparameter optimization.\",\n    conda_dependencies=[\"xgboost\"],\n    sample_input_data=train_df.drop(\"SHIFT_SALES\"),\n    options={'relax_version': False}\n)\n\n# Add evaluation metric\nmodel_ver.set_metric(metric_name=\"mean_abs_pct_err\", value=optimal_mape)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SnowflakeModelRegistry_ShowVersions_Code",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Let's confirm model(s) that were added\nregistry.get_model(model_name).show_versions()",
   "id": "a4875ffe-5d80-4bde-b3fa-2b295f3abff9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SnowflakeModelRegistry_ShowDefaultModel_Code",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# We can see what the default model is when we have multiple versions with the same model name:\nregistry.get_model(model_name).default.version_name",
   "id": "d511dc1c-9cb1-4572-b90d-6940eebfdf49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SnowflakeModelRegistry_ModelInference_Code",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Now we can use the default version model to perform inference.\nmodel_ver = registry.get_model(model_name).version('V0')\nresult_sdf = model_ver.run(test_df, function_name=\"predict\")\nresult_sdf.show()",
   "id": "462fcbcc-479b-47b8-a99a-6e7cc518276d"
  },
  {
   "cell_type": "markdown",
   "id": "b6853926-5ee3-46c0-9856-661755ea3451",
   "metadata": {
    "name": "ViewRegisteredModels",
    "collapsed": false
   },
   "source": "## View Registered Models\n\nLet's look at the Snowflake Model Registry UI. It is a unified place in Snowsight UI where all models registered in the Model Registry can be found and explored. \n\nTo navigate to the Model Registry, click on “AI & ML” in the left-nav, and select “Models”. This will display all available models in the Model Registry across all DBs/Schemas that your role has access to.\n\nCurrently, Model Registry will display user-created models from Snowpark ML, Snowpark, or models sourced from external ML platforms that are registered in Snowflake. In the future, Model Registry will also support other model types such as Cortex ML, Doc AI, Cortex Fine Tuning etc. \n\nTo explore the details of a model, click on the corresponding row in the listing. This will open the details page for the selected model as shown below. \n\nKey model details such as description, any tags that are applied to the model, and a table of all versions of the model are displayed. \n\nModel Version metadata such as metrics can be found here. This page also shows the available model methods for invocation along with the inputs and outputs of those functions.\n"
  },
  {
   "cell_type": "code",
   "id": "5210af9f-1e35-45e2-a813-040935b2fc44",
   "metadata": {
    "language": "python",
    "name": "ModelRegistryUI",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "image1=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.ASSETS/ModelRegistered.png\" , decompress=False).read() \nimage2=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.ASSETS/Model.png\" , decompress=False).read() \n\n# Display the image\nst.image(image1, width=1000)\nst.image(image2, width=1000)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9def2b43-3cf2-4772-bdde-9e8484a157b8",
   "metadata": {
    "name": "TestModel",
    "collapsed": false
   },
   "source": "Now that our model is built and deployed, let's see it in action! We will find the best place to park in Vancouver for tomorrow morning's shift."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "SnowflakeModelRegistry_ModelInferenceCheck_Code",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Check model predictions for holdout data SHIFT_SALES predictions for Location_IDs in Vancouver\ndate_tomorrow_df = oe_df.filter(\n    (F.col(\"shift_sales\").isNull())\n    & (F.col(\"shift_oe\") == 1)\n    & (F.col(\"city\") == \"Vancouver\")\n)\nresult_sdf = regressor.predict(date_tomorrow_df)\nresult_sdf.show()",
   "id": "4b34593d-35ea-4a93-8c46-e8d871aa5a2d"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "SnowflakeModelRegistry_ModelInferenceCheck_SiS"
   },
   "source": [
    "## Visualize on a Map\n",
    "\n",
    "The red and yellow areas indicate higher predicted sales locations and the green zones indicate lower predicted sales. We will use this insight to ensure that our drivers are parking at the high-value locations. Value: Updated predictions readily available to drive towards our corporate goals.\n",
    "\n"
   ],
   "id": "88624ec7-88ae-4ce4-b3d7-2322b6e374f0"
  },
  {
   "cell_type": "code",
   "id": "b50ec110-4223-4d70-b46d-4e8622fcf462",
   "metadata": {
    "language": "python",
    "name": "SnowparkML_VisualizePredictions_SiS_Code",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Pull location predictions into a pandas DataFrame\npredictions_df = result_sdf.to_pandas()\npredictions_df.head()\n\n# Visualize on a map\nst.map(predictions_df)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Part4_SiSApp",
    "collapsed": false
   },
   "source": "# Part 4 - Create a SiS application to use predicted outputs\n\nCreate a SiS application for local managers to identify where to place daily food trucks. \n\nSee completed [SiS App](https://app.snowflake.com/sfsenorthamerica/sfhol/#/streamlit-apps/HOL.SCHEMA0.ZXGOF2KZL26CGAA8?ref=snowsight_shared) using role=PUBLIC\n\n",
   "id": "fe5ff0fb-e2c7-4aeb-b9e7-805cfae17424"
  },
  {
   "cell_type": "code",
   "id": "de5f573c-e7a9-4ac6-87dd-9261c2f958f1",
   "metadata": {
    "language": "python",
    "name": "Part4",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "image1=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.ASSETS/Part4.png\" , decompress=False).read() \nimage2=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.ASSETS/SiSapp.png\" , decompress=False).read() \n\n# Display the image\nst.image(image1, width=600)\n\nst.subheader(\"Here's a picture of the running SiS app:\")\nst.image(image2, width=600)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "011b641a-958b-47fa-99ae-d0e83e4f2277",
   "metadata": {
    "name": "Part4_SiS",
    "collapsed": false
   },
   "source": "- Create new SiS app.\n- Replace existing code with SiS_application.py code listed below.\n- Update schema variable in line 29.\n- Include required pydeck and snowflake-ml-python package.\n- Run the SiS application."
  },
  {
   "cell_type": "markdown",
   "id": "38d24cfb-06c3-453a-8539-daeb8fac1747",
   "metadata": {
    "name": "SiSApp",
    "collapsed": false
   },
   "source": "```\n# Import Python packages\nimport streamlit as st\nimport pydeck as pdk\nimport numpy as np\n\n# Import Snowflake modules\nfrom snowflake.snowpark import Session\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.ml.registry import Registry\nimport snowflake.ml.modeling.preprocessing as snowmlpp\nfrom snowflake.ml.feature_store import FeatureStore\n\n# Set Streamlit page config\nst.set_page_config(\n    page_title=\"Streamlit App: Snowpark 101\", \n    page_icon=\":truck:\",\n    layout=\"wide\",\n)\n\n# Add header and a subheader\nst.header(\"Predicted Shift Sales by Location\")\nst.subheader(\"Data-driven recommendations for food truck drivers.\")\n\n# Connect to Snowflake\nsession = get_active_session()\n\n# Create input widgets for cities and shift\nwith st.container():\n    col1, col2 = st.columns(2)\n    with col1:\n        # Drop down to select city\n        city = st.selectbox(\n            \"City:\",\n            session.table(f\"DATASCIENCECOLLEGE.PUBLIC.SHIFT_SALES\")\n            .select(\"city\")\n            .distinct()\n            .sort(\"city\"),\n        )\n\n    with col2:\n        # Select AM/PM Shift\n        shift = st.radio(\"Shift:\", (\"AM\", \"PM\"), horizontal=True)\n\n    n_trucks = st.selectbox('How many food trucks would you like to schedule today?', np.arange(1,10))\n\n    if n_trucks > 1:\n        range = st.slider('What is the minimum distance in kilometers between food trucks?', 0, 20, 1)\n        st.write('You are requesting a minimum distance of ', range, 'km')\n        st.write('Click **:blue[Update]** to get the ', n_trucks, ' highest predicted Shift_Sales food truck locations.')\n    else:\n        st.write('Click **:blue[Update]** to get one food truck location predicted to have the Shift_Sales')\n        \n# Get predictions for city and shift time\ndef get_predictions(city, shift):\n    # Connect to the Feature Store\n    fs = FeatureStore(\n        session=session,\n        database=\"DATASCIENCECOLLEGE\",\n        name=\"PUBLIC\"\n    )\n    \n    # Retrieve the feature view from the Feature Store\n    feature_view = fs.get_feature_view(\n        name=\"AGGREGATE_WINDOW_FV\", \n        version=\"1\"\n    )\n    \n    # Get data and filter by city and shift\n    snowpark_df = feature_view.to_snowpark_df().filter((F.col(\"SHIFT\") == shift) & (F.col(\"CITY\") == city))\n\n    # Get tomorrow's date\n    date_tomorrow = (\n        snowpark_df.filter(F.col(\"SHIFT_SALES\").is_null())\n        .select(F.min(\"DATE\"))\n        .collect()[0][0]\n    )\n\n    # Filter to tomorrow's date\n    snowpark_df = snowpark_df.filter(F.col(\"DATE\") == date_tomorrow)\n\n    # Impute\n    snowpark_df = snowpark_df.fillna(value=0, subset=[\"AVG_LOCATION_SHIFT_SALES\"])\n\n    for colname in snowpark_df.columns:\n        new_colname = str.upper(colname)\n        snowpark_df = snowpark_df.with_column_renamed(colname, new_colname)\n\n    # Encode\n    snowpark_df = snowpark_df.with_column(\"SHIFT_OE\", F.iff(F.col(\"SHIFT\") == \"AM\", 0, 1))\\\n    .with_column(\"SHIFT_OE\", F.iff(F.col(\"SHIFT\") == \"PM\", 1, 0))\n\n    # Scale\n    mm_target_columns = [\"CITY_POPULATION\"]\n    mm_target_cols_out = [\"CITY_POPULATION_NORM\"]\n    snowml_mms = snowmlpp.MinMaxScaler(input_cols=mm_target_columns, \n                                       output_cols=mm_target_cols_out)\n    snowml_mms.fit(snowpark_df)\n    snowpark_df = snowml_mms.transform(snowpark_df)\n\n    # Get all features\n    feature_cols = [\"SHIFT_OE\", \n                    \"CITY_POPULATION_NORM\", \n                    \"MONTH\", \n                    \"DAY_OF_WEEK\",\n                    \"LATITUDE\",\n                    \"LONGITUDE\",\n                    \"AVG_LOCATION_SHIFT_SALES\",\n                    \"LOCATION_ID\"]\n\n    snowpark_df = snowpark_df.select(feature_cols)\n    native_registry = Registry(session=session, database_name=\"DATASCIENCECOLLEGE\", schema_name=\"PUBLIC\")\n    model_ver = native_registry.get_model(\"SHIFT_SALES_PREDICTION\").version('v0')\n    result_sdf = model_ver.run(snowpark_df, function_name=\"predict\")\n    return result_sdf\n\n# Update predictions and plot when the \"Update\" button is clicked\nif st.button(\":blue[Update]\"):\n    # Get predictions\n    with st.spinner(\"Getting predictions...\"):\n        predictions_sdf = get_predictions(city, shift)\n        predictions = predictions_sdf.to_pandas()\n\n    # Plot on a map\n    st.subheader(\"Predicted Shift Sales for position\")\n    predictions[\"PRED_SHIFT_SALES\"].clip(0, inplace=True)\n    st.pydeck_chart(\n        pdk.Deck(\n            map_style=None,\n            initial_view_state=pdk.ViewState(\n                latitude=predictions[\"LATITUDE\"][0],\n                longitude=predictions[\"LONGITUDE\"][0],\n                zoom=11,\n                pitch=50,\n            ),\n            layers=[\n                pdk.Layer(\n                    \"HexagonLayer\",\n                    data=predictions,\n                    get_position=\"[LONGITUDE, LATITUDE]\",\n                    radius=200,\n                    elevation_scale=4,\n                    elevation_range=[0, 1000],\n                    pickable=True,\n                    extruded=True,\n                ),\n                pdk.Layer(\n                    \"ScatterplotLayer\",\n                    data=predictions,\n                    get_position=\"[LONGITUDE, LATITUDE]\",\n                    get_color=\"[200, 30, 0, 160]\",\n                    get_radius=200,\n                ),\n            ],\n        )\n    )\n\n    max_x = predictions.loc[predictions[\"PRED_SHIFT_SALES\"].idxmax()]\n    st.write(\"Maximum Predicted Sales are expected at the following location:\", max_x)\n\n    #st.dataframe(predictions_sdf)\n    location_id = max_x[\"LOCATION_ID\"]\n    lat = max_x[\"LATITUDE\"]\n    long = max_x[\"LONGITUDE\"]\n    st.subheader(\"The following chart is generated using the st_point and st_distance Snowflake Geospatial features\")\n\n    if n_trucks == 1:\n        st.write(\"Have your only food truck positioned at Location ID \", location_id, \" to maximize SHIFT_SALES\")\n    elif n_trucks > 1:\n        best_locations = [location_id]\n        available_locations_sdf = predictions_sdf\n        st_distance = F.function('st_distance')\n        st_point = F.function('st_point')\n        for truck_n in np.arange(0,n_trucks - 1):\n            available_locations_sdf = available_locations_sdf.with_column(\"DISTANCE_TO_TRUCK\", \n                                        st_distance(\n                                            st_point(F.lit(float(long)), F.lit(float(lat))),\n                                            st_point(F.col(\"LONGITUDE\"), F.col(\"LATITUDE\"))\n                                        )/1609\n                                       ).filter(F.col(\"DISTANCE_TO_TRUCK\") >= range/1.609).order_by(\"PRED_SHIFT_SALES\", ascending=False)\n            max_x = available_locations_sdf.limit(1).to_pandas()\n            try:\n                location_id = max_x[\"LOCATION_ID\"].iloc[0]\n                lat = max_x[\"LATITUDE\"].iloc[0]\n                long = max_x[\"LONGITUDE\"].iloc[0]\n            except:\n                break\n            best_locations.append(location_id)\n\n        selected_locations = predictions[predictions[\"LOCATION_ID\"].isin(best_locations)]\n        st.map(selected_locations)\n        st.dataframe(selected_locations)\n```"
  }
 ]
}